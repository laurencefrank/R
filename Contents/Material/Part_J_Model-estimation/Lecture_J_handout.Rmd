---
title: "Linear Modeling"
author: "Gerko Vink"
date: "Statistical Programming with R"
output: 
  html_document:
    toc: true
    toc_depth: 5
    toc_float: true
    number_sections: false
---
<style type="text/css">

body{ /* Normal  */
      font-size: 12px;
  }
td {  /* Table  */
  font-size: 12px;
}
h1.title {
  font-size: 18px;
  color: DarkBlue;
}
h1 { /* Header 1 */
  font-size: 18px;
}
h2 { /* Header 2 */
    font-size: 18px;
}
h3 { /* Header 3 */
  font-size: 18px;
}
code.r{ /* Code block */
    font-size: 12px;
}
pre { /* Code block - determines code spacing between lines */
    font-size: 14px;
}
</style>

---

##  We use the following packages
```{r message=FALSE}
library(MASS)
library(dplyr)
library(magrittr)
library(ggplot2)
library(mice)
library(DAAG)
library(car)
```

# Fitting a line to data

##  Linear regression
Linear regression model
\[
y_i=\alpha+\beta{x}_i+\varepsilon_i
\]

Assumptions:

  -  $y_i$ conditionally normal with mean $\mu_i=\alpha+\beta{x}_i$
  -  $\varepsilon_i$ are $i.i.d.$ with mean 0 and (constant) variance $\sigma^2$

## The `anscombe` data
```{r}
head(anscombe)
```

Or, alternatively, 
```{r}
anscombe %>%
  head
```

## Fitting a line {.smaller}

```{r eval = FALSE, message = FALSE}
anscombe %>%
  ggplot(aes(y1, x1)) + 
  geom_point() + 
  geom_smooth(method = "lm")
```

##  Fitting a line {.smaller}

```{r echo=FALSE, message = FALSE}
anscombe %>%
  ggplot(aes(y1, x1)) + 
  geom_point() + 
  geom_smooth(method = "lm")
```

##  Fitting a line {.smaller}
The linear model would take the following form:
```{r eval=FALSE}
fit <- 
  yourdata %>%
  lm(youroutcome ~ yourpredictors)

summary(fit)
```
Output:

-  Residuals: minimum, maximum and quartiles
-  Coefficients: estimates, SE's, t-values and $p$-values
-  Fit measures
    -  Residuals SE (standard error residuals)
    -  Multiple R-squared (proportion variance explained)
    -  F-statistic and $p$-value (significance test model)

##  `anscombe` example {.smaller}
```{r message=FALSE, warning = FALSE}
fit <- anscombe %$%
  lm(y1 ~ x1)

summary(fit)
```

## Checking assumptions

1. linearity
    - scatterplot $y$ and $x$ 
    - include loess curve when in doubt
    - does a squared term improve fit?
2. normality residuals
    -  normal probability plots `qqnorm()`
    -  if sample is small; `qqnorm` with simulated errors cf. `rnorm(n, 0, s)` 
3. constant error variance 
    -  residual plot
    -  scale-location plot

## Linearity {.smaller}
```{r eval = FALSE, message = FALSE}
anscombe %>%
  ggplot(aes(x1, y1)) + 
  geom_point() + 
  geom_smooth(method = "loess", col = "blue") + 
  geom_smooth(method = "lm", col = "orange")
```

## Linearity {.smaller}
```{r echo=FALSE, message = FALSE}
anscombe %>%
  ggplot(aes(x1, y1)) + 
  geom_point() + 
  geom_smooth(method = "loess", col = "blue") + 
  geom_smooth(method = "lm", col = "orange")
```

The loess curve suggests slight non-linearity

## Adding a squared term
```{r message=FALSE, warning = FALSE}
anscombe %$%
  lm(y1 ~ x1 + I(x1^2)) %>%
  summary()
```

## Constant error variance? {.smaller}
```{r, fig.height=4, dev.args = list(bg = 'transparent')}
par(mfrow = c(1, 2))
fit %>%
  plot(which = c(1, 3), cex = .6)
```

## No constant error variance! {.smaller}
```{r, fig.height=4, dev.args = list(bg = 'transparent')}
par(mfrow = c(1, 2))
boys %$%
  lm(bmi ~ age) %>%
  plot(which = c(1, 3), cex = .6)
```

## Normality of errors {.smaller}
```{r, fig.height=4, dev.args = list(bg = 'transparent')}
fit %>%
  plot(which = 2, cex = .6)
```

The QQplot shows some divergence from normality at the tails

# Outliers, influence and robust regression

## Outliers and influential cases {.smaller}

Leverage: see the fit line as a lever. 
  
  - some points pull/push harder; they have more leverage
  
Standardized residuals:

  - The values that have more leverage tend to be closer to the line
  - The line is fit so as to be closer to them
  - The residual standard deviation can differ at different points on $X$ - even if the error standard deviation is constant. 
  - Therefore we standardize the residuals so that they have constant variance (assuming homoscedasticity). 

Cook's distance: how far the predicted values would move if your model were fit without the data point in question. 

  - it is a function of the leverage and standardized residual  associated with each data point

```{r echo = FALSE}
set.seed(20)

pred1 = rnorm(20, mean=20, sd=3)
outcome1 = 5 + .5*pred1 + rnorm(20)

pred2 = c(pred1, 30);        outcome2 = c(outcome1, 20.8)
pred3 = c(pred1, 19.44);     outcome3 = c(outcome1, 20.8)
pred4 = c(pred1, 30);        outcome4 = c(outcome1, 10)
```

## Fine
```{r echo=FALSE}
par(mfrow = c(1,2))
plot(outcome1 ~ pred1, ylim = c(9, 25), xlim = c(10, 30))
abline(lm(outcome1 ~ pred1))
plot(lm(outcome1 ~ pred1), which = 5)
```

## High leverage, low residual
```{r echo=FALSE}
par(mfrow = c(1,2))
plot(outcome2 ~ pred2, ylim = c(9, 25), xlim = c(10, 30))
points(30, 20.8, col = "red")
abline(lm(outcome2 ~ pred2))
plot(lm(outcome2 ~ pred2), which = 5)
```

## Low leverage, high residual
```{r echo=FALSE}
par(mfrow = c(1,2))
plot(outcome3 ~ pred3, ylim = c(9, 25), xlim = c(10, 30))
abline(lm(outcome3 ~ pred3))
points(19.44, 20.8, col = "red")
plot(lm(outcome3 ~ pred3), which = 5)
```

## High leverage, high residual
```{r echo=FALSE}
par(mfrow = c(1,2))
plot(outcome4 ~ pred4, ylim = c(9, 25), xlim = c(10, 30))
points(30, 10, col = "red")
abline(lm(outcome4 ~ pred4))
plot(lm(outcome4 ~ pred4), which = 5)
```

## Outliers and influential cases
Outliers are cases with large $e_z$  (standardized residuals).

If the model is ***correct***  we expect:
  
  -  5\% of $|e_z|>1.96$ 
  -  1\% of $|e_z|>2.58$
  -  0\% of $|e_z|>3.3$ 

Influential cases are cases with large influence on parameter estimates
  
  -  cases with Cook's Distance $> 1$, or 
  -  cases with Cook's Distance much larger than the rest

## Outliers and influential cases
```{r, fig.height= 3, dev.args = list(bg = 'transparent')}
par(mfrow = c(1, 2), cex = .6)
fit %>% plot(which = c(4, 5))
```

There are no cases with $|e_z|>2$, so no outliers (right plot). There are no cases with Cook's Distance $>1$, but case 3 stands out 

# Model fit

## New data example
```{r cache = TRUE}
boys.fit <- 
  na.omit(boys) %$% # Extremely wasteful
  lm(age ~ reg)
```

```{r}
boys %>% 
  head
```


## Model {.smaller}
```{r cache = TRUE}
boys.fit %>%
  anova()
```

##
```{r cache = TRUE}
boys.fit %>%
  summary()
```

## Model factors
```{r cache = TRUE}
boys.fit %>%
  model.matrix() %>%
  head()
```

## `aov()`
```{r cache = TRUE}
boys.fit %>% 
  aov()
```

`aov()` is for balanced designs.

## `anova()`
```{r cache = TRUE}
boys.fit %>% 
  anova()
```

## `Anova()` with type I Sum of Squares

1. SS(A) for factor A.
2. SS(B | A) for factor B.
3. SS(AB | B, A) for interaction AB.

- If data are unbalanced, the ordering defines the estimates:
  - Different ordering of factors will yield different effects 

## `Anova()` with type II Sum of Squares

1. SS(A | B) for factor A.
2. SS(B | A) for factor B.

No interaction assumed. So test for this first!

- only if interaction is not significant continue with interpretation of the main effects. 
- most powerful if there is no interaction

## `Anova()` with type II Sum of Squares
```{r cache = TRUE}
boys.fit %>% 
  car::Anova(type = 2)
```

## `Anova()` with type III Sum of Squares

1. SS(A | B, AB) for factor A.
2. SS(B | A, AB) for factor B.

Tests if there is a main effect, given the other main effect and the interaction. 

- however, most often we do not care about interpretation of the main effects if the interaction is significant

`When data are balanced, Type I, II and II are identical, because the factors are orthogonal`

## `Anova()` with type III Sum of Squares
```{r cache = TRUE}
boys.fit %>% 
  car::Anova(type = 3)
```

## Post hoc comparisons
```{r cache = TRUE}
coef <- 
  boys.fit %>%
  aov() %>%
  summary.lm()
coef
```

## Post hoc comparisons
```{r cache = TRUE}
p.val <- coef$coefficients
p.adjust(p.val[, "Pr(>|t|)"], method = "bonferroni")
```

## AIC and BIC
Akaike's *An Information Criterion* 
```{r cache = TRUE}
boys.fit %>% 
  AIC()
```

and *Bayesian Information Criterion*
```{r cache = TRUE}
boys.fit %>%
  BIC()
```

## Model comparison
```{r cache = TRUE}
boys.fit2 <- 
  na.omit(boys) %$%
  lm(age ~ reg + hgt)

boys.fit %>% AIC()
boys.fit2 %>% AIC()
```

## Another model
```{r cache = TRUE}
boys.fit3 <- 
  na.omit(boys) %$%
  lm(age ~ reg + hgt * wgt)
```
is equivalent to 
```{r eval=FALSE}
boys.fit3 <- 
  na.omit(boys) %$%
  lm(age ~ reg + hgt + wgt + hgt:wgt)
```

## Model comparison
```{r cache = TRUE}
boys.fit %>% AIC()
boys.fit2 %>% AIC()
boys.fit3 %>% AIC()
```

## Model comparison
```{r cache = TRUE}
anova(boys.fit, boys.fit2, boys.fit3)
```

## Model fit again
```{r cache = TRUE}
boys.fit3 %>%
  car::Anova(type = 3)
```

## Model fit again
```{r cache = TRUE}
boys.fit3 %>%
  car::Anova(type = 2)
```

## Model fit again
```{r cache = TRUE}
boys.fit3 %>%
  anova()
```

## Influence of cases
DfBeta calculates the change in coefficients depicted as deviation in SE's.
```{r cache = TRUE}
boys.fit3 %>%
  dfbeta() %>%
  head(n = 7)
```

# Prediction

## Fitted values:
Let's use the simpler `anscombe` data example
```{r cache = TRUE}
y_hat <- 
  fit %>%
  fitted.values()
```
The residual is then calculated as
```{r cache = TRUE}
y_hat - anscombe$y1
```

## Predict new values
If we introduce new values for the predictor `x1`, we can generate predicted values from the model
```{r cache = TRUE, warning=FALSE}
new.x1 <- data.frame(x1 = 1:20)
predict.lm(fit, newdata = new.x1)
```

## Predictions are draws from the regression line
```{r}
pred <- predict.lm(fit, newdata = new.x1)
lm(pred ~ new.x1$x1)$coefficients
fit$coefficients
```

## Prediction intervals
```{r warning=FALSE}
predict(fit, interval = "prediction")
```

# Assessing predictive accuracy

## K-fold cross-validation

- Divide sample in $k$ mutually exclusive training sets
- Do for all $j\in\{1,\dots,k\}$ training sets
  
    1. fit model to training set $j$
    2. obtain predictions for test set $j$  (remaining cases)
    3. compute residual variance (MSE) for test set $j$
  
- Compare MSE in cross-validation with MSE in sample
- Small difference suggests good predictive accuracy

## K-fold cross-validation `anscombe` data {.smaller}

  -  residual variance sample is $1.24^2 \approx 1.53$
  -  residual variance cross-validation is 1.93 
  -  regression lines in the 3 folds are similar

```{r echo=F, fig.height=4, dev.args = list(bg = 'transparent')}
par(mar=c(5,4,4,2)+.1,cex=.8,cex.lab=1.5,cex.axis=1.2,cex.main=1.5)
```
```{r echo=T, fig.height=3.5, dev.args = list(bg = 'transparent')}
DAAG::CVlm(anscombe, fit, plotit=T, printit=F)
```

## K-fold cross-validation `boys` data {.smaller}

  -  residual variance sample is 2
  -  residual variance cross-validation is 2.38
  -  regression lines in the 3 folds almost identical

```{r echo=F, fig.height=4, dev.args = list(bg = 'transparent')}
par(mar=c(5,4,4,2)+.1,cex=.8,cex.lab=1.5,cex.axis=1.2,cex.main=1.5)
```
```{r echo=T, fig.height=3.5, dev.args = list(bg = 'transparent'), warning=FALSE}
DAAG::CVlm(na.omit(boys), boys.fit3, plotit=T, printit=F)
```

## How many cases are used?
```{r}
na.omit(boys) %$%
  lm(age ~ reg + hgt * wgt) %>%
  nobs()

```

If we would not have used `na.omit()`
```{r}
boys %$%
  lm(age ~ reg + hgt * wgt) %>%
  nobs()
```

## Some other modeling devices in `R`
- `lm()`: linear modeling
- `glm()`: generalized linear modeling
- `gamlss::gamlss`: generalized additive models (for location, scale and shape)
- `lme4::lmer`: linear mixed effect models
- `lme4::glmer`: generalized linear mixed effect models
- `lme4::nlmer`: non-linear mixed effect models

