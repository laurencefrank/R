---
title: "Generalized linear models"
author: "Maarten Cruyff"
date: "Statistical Programming with R"
output:
  ioslides_presentation:
    logo: logo.png
    smaller: yes
    widescreen: no
---


```{r include = FALSE, message = FALSE}
knitr::opts_chunk$set(comment = "", message = FALSE)
library(dplyr)
library(ggplot2)
```


## Content

1. Generalized linear model (GLM)

    - variables with non-normal error distribution
    
    - families and link functions
    
2. Dichotomous variables

    - binomial family
    
    - logit link function
    
    - logistic regression

3. Fit measures

    - Deviance vs $R^2$


## $Y$ is continuous

The linear model applies to a **continuous** dependent variable $Y$.

<br>

$$\mu=\beta_0+\beta{x}+\varepsilon, \ \ \ \ \ \varepsilon\sim{N}(0,\sigma^2)$$

<br>

- $\mu$ is the mean of $Y$ given the score on $X$.

- Residuals are normally distributed and homoscedastic.

<br>

<center>

![](pics/errors.png)

</center>


## $Y$ is dichotomous

Dichotomous variables

    - pass/fail the exam (pass = 1, fail = 0)
    
    - smoker/non-smoker (smoker = 1, non-smoker = 0)

<br>

Predict the probability $\mu=P(Y=1)$ with linear model

<br>

$$\mu=\beta_0+\beta{x}+\varepsilon, \ \ \ \ \ \varepsilon\sim{Bin}(n,p)$$

<br>

Problem:

- binomial error distribution (non-normal and heteroscedastic)

- estimates outside the inetrval $(0,1)$


## Example passing the an exam

Distribution of variable `pass` for 100 students

- 80 students passed the exam $(pass = 1)$

- 20 students failed the exam $(pass = 0)$

```{r include = TRUE, echo = FALSE, fig.align = "center", fig.width = 4, fig.height = 4}
set.seed(2)
pass  <- rep(0:1, c(20, 80))
study <- rnorm(100, pass + 2, 0.3)
barplot(table(pass))
```


## Predictions from the linear model

Predict passing the exam for study time.

```{r include = TRUE, echo = FALSE, fig.width = 4, fig.height = 3, fig.align = "center"}
ggplot(data.frame(pass, study), aes(study, pass)) +
  geom_point() +
  geom_smooth(method = "lm") +
  theme_minimal()
```


## Residual plots

```{r include = TRUE, echo = FALSE, fig.width = 5, fig.height = 5, fig.align = "center"}
p <- lm(pass ~ study)

par(mfrow = c(2, 2))
plot(p)
```


## Generalized Linear Model

The GLM does not predict $\mu$ but a function of $\mu$.

- The function $g(\mu)$ is called the link function.

<br>

$$g(\mu)=\beta_0+\beta{x}+\varepsilon$$

<br>

The link function ensures that predictions are within the permitted range.

- between 0 and 1 for dichotomous variables

<br>

The GLM does not assume normality or homoscedasticity.


## Families of distributions

The GLM distinguishes various families of distributions, e.g.:

- gaussian family for continuous variables

- binomial family for dichotomous variables

<br>

family | DV  | link  | $g(\mu)=\beta_0+\beta{x}$ | $\mu=g^{-1}(\beta_0+\beta{x})$
---------|--------|-----------|------------|-------------
gaussian | continuous |identity  | $g(\mu)=\mu$       | $\mu=\beta_0+\beta{x}$
binomial | dichotomous | logit     | $g(\mu)=\log\frac{\mu}{1-\mu}$  | $\mu=\frac{\exp(\beta_0+\beta{x})}{1+\exp(\beta_0+\beta{x})}$


## Fitting a GLM

As `lm()`, but with additional family argument:

```{r eval = FALSE}
glm(formula, family = c("gaussian", "binomial"), data)
```

<br>

Predictions

```{r eval = FALSE}
predict(object, type = c("link", "response")
```

<br>

- `object` is a fitted GLM model

- `type = "link"` for prediction of the linear predictor $g(\mu)=\beta_0+\beta{x}$ 

- `type = "response"` for prediction of the mean $\mu=g^{-1}(\beta_0+\beta{x})$ 




## Probability of passing the exams 

Logistic regression model

```{r echo = FALSE}
pass.glm <- glm(pass ~ study, family = binomial)
summary(pass.glm)
```

## Interpretation parameter estimates

Parameter estimates are on the logit scale

$$logit(pass)=\log\frac{\mu}{1-\mu}=\beta_0+\beta{x}$$

<br>

Probability estimates are obtained via the inverse link):

$$P(pass)=\frac{\exp(\beta_0+\beta{x})}{1+\exp(\beta_0+\beta{x})}$$

<br>

Notice that $P(pass)$ is always between 0 and 1!

## Example

Logit for student who studied 2 hrs

$$logit(pass)=\beta_0+2.5\beta_{study}=-25.6 + 2.5\times10.8\approx  0$$
<br>

Probility to pass:

$$P(pass)=\frac{\exp(\beta_0+2.5\beta_{study})}{1+\exp(\beta_0+2.5\beta_{study})}=\frac{\exp(0)}{1+\exp(0)}=\frac{1}{2}$$
## Regression line

```{r echo = FALSE, fig.width = 4, fig.height = 3, fig.align = "center"}
d <- data.frame(pass, study, pred = predict(pass.glm, type = "response"))

ggplot(d) +
  geom_line(aes(study, pred), col="blue", size = 1) +
  geom_point(aes(study, pass)) + labs(y = "P(pass)") +
  theme_minimal()
```


## Fit measures `lm()`

The linear model uses the $F$ and $R^2$ statistics

```{r echo = FALSE}
summary(lm(am ~ disp, mtcars))
```

## Fit measures `glm()`

The linear model uses the $F$ and $R^2$ statistics

```{r echo = FALSE}
summary(glm(am ~ disp, binomial, mtcars))
```

## Deviance and AIC

Deviance is measure for difference between observed and fitted values


- `Null deviance`: deviance of intercept-only model


- `Residual Deviance`: deviance of fitted model

<br>

The larger the difference, the better the model

<br>

- `AIC`: model with the lowest AIC is the most parsimonious

## Example

Intercept-only model

```{r echo = FALSE}
glm(pass ~ 1, binomial)
```
Model with predictor `study`
```{r echo = FALSE}
glm(pass ~ study, binomial)
```

## Summary

Generalized Linear Model

- variables with non-normal error distribution

- family and link function

- inverse link function (`type = "response") is on the original scale of the DV

- fit is measured by the deviance and AIC
  