---
title: 'Bootstrapping'
author: "Erik-Jan van Kesteren & Gerko Vink"
date: "Statistical Programming with R"
output:
  ioslides_presentation:
    logo: logo.png
    smaller: yes
    widescreen: no
  beamer_presentation:
    colortheme: beaver
---

```{r setup, echo=FALSE, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(dev        = "png",
                      dev.args   = list(bg = "transparent"), 
                      fig.align  = "center",
                      message    = FALSE, 
                      warning    = FALSE)
RNGkind(sample.kind = "Rounding")
```

## We use the following packages
```{r pkgs}
library(dplyr)    # Data manipulation
library(magrittr) # Pipes
library(ggplot2)  # Plotting suite
library(boot)     # Bootstrapping functions
```
<img src="munchausen.svg" style="display:block;width:35%;margin:0 auto;"></img>

## Running example

- a country called sumR 
- 3 million adult humans inhabit it
- 3 cities Rnhem, AlkmR, and LeeuwRden
- height
- shoe size

```{r}
set.seed(54321)
 
height   <- rnorm(3e6, mean = 180, sd = 7.5)
shoesize <- 35 + 0.02*height + runif(3e6, -1, 1)
city     <- sample(c("Rnhem", "AlkmR", "LeeuwRden"), size = 3e6, 
                   prob = c(0.6, 0.3, 0.1), replace = TRUE)

sumR <- data.frame(height, shoesize, city)
```

## Running example
```{r, fig.width=8, fig.height=3}
par(mfrow = c(1,3))
hist(height,   breaks = 80, col = "light green", border = "transparent")
hist(shoesize, breaks = 80, col = "light blue",  border = "transparent")
table(city) %>% barplot(col = "pink")
```

(In a real country we will not know these things)

## Let's do statistics!

We randomly sample 100 persons and "measure" their height, shoesize and city:
```{r}
samp100 <- sample(3e6, size = 100)
sdat100 <- sumR[samp100, ]
head(sdat100)
```

Let's use this sample to estimate quantities and infer things about our population!

## Goal

We want to know the mean height of our population

The sample mean $\bar{x}$ is an estimator for the population mean $\mu$
```{r}
m <- mean(sdat100$height)
m
```

# The sampling distribution

## The sampling distribution: parametric
This statistic has a __sampling distribution__: taking a different sample generates a different mean value

Thus we have __uncertainty__ about our estimated mean

```{r, echo=FALSE, fig.height=4}
x <- seq(m - 7, m + 7, len = 1000)
p <- 
  data.frame(x, y = dnorm(x = x, 180, .75)) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_polygon(fill = "light blue", alpha = 0.7) +
  geom_line(lwd = 1, col = "dark blue") +
  geom_vline(xintercept = m, lty = 3) +
  labs(x = "Mean height", y = "Density", 
       title = "The sampling distribution",
       fill = "") +
  theme_minimal()

plot(p)
```


## The sampling distribution: parametric

- The sampling distribution of the mean is a normal distribution with _parameters_: 
    - mean $\mu_\bar{x}$ 
    - standard deviation $\sigma_\bar{x}$
- We can estimate these quantities
- Mean estimator: sample mean $$\hat{\mu}_\bar{x} = \bar{x}$$ 
- Standard deviation estimator: the standard error $$\hat{\sigma}_\bar{x} = \frac{s_x}{\sqrt{n}}$$

```{r}
se <- sd(sdat100$height)/sqrt(100)
se
```

## The sampling distribution: parametric

```{r, echo=FALSE, fig.height=2.7}
x <- seq(m - 7, m + 7, len = 1000)
p <- 
  data.frame(x, y = dnorm(x = x, 180, .75)) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_polygon(fill = "light blue", alpha = 0.7) +
  geom_line(lwd = 1, col = "dark blue") +
  geom_vline(xintercept = m, lty = 3) +
  labs(x = "Mean height", y = "Density", 
       title = "The sampling distribution",
       fill = "") +
  theme_minimal()

plot(p)
```

```{r, echo=FALSE, fig.height=2.7}
p <- 
  data.frame(x, y = dnorm(x = x, m, se)) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_polygon(fill = "light green", alpha = 0.7) +
  geom_line(lwd = 1, col = "dark green") +
  geom_vline(xintercept = m, lty = 3) +
  labs(x = "Mean height", y = "Density", 
       title = "Estimate of the sampling distribution",
       fill = "") +
  theme_minimal()

plot(p)
```

## Parametric Confidence Interval
Using the standard error and the assumption of normality, we can create a confidence interval (CI):

```{r ci}
(ci <- c(m - 1.96*se, m + 1.96*se))
```
```{r, echo=FALSE, fig.height=3.5}
p <- 
  data.frame(x, y = dnorm(x = x, m, se)) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_polygon(fill = "light green", alpha = 0.7) +
  geom_line(lwd = 1, col = "dark green") +
  geom_vline(xintercept = m, lty = 3) +
  geom_vline(xintercept = ci[1]) +
  geom_vline(xintercept = ci[2]) +
  labs(x = "Mean height", y = "Density", 
       title = "95% Confidence interval",
       fill = "") +
  theme_minimal()

plot(p)
```


## The sampling distribution: parametric

### Interim conclusion

- Any sample statistic has __uncertainty__ due to sampling
- This uncertainty is captured in the __sampling distribution__
- Asymptotically (as sample size grows) we assume it is a __normal distribution__
- The __standard error__ is a measure of uncertainty
- Using the standard error, we can create a __95% CI__


## The sampling distribution: algorithmic

### Let's take a step back
- The sampling distribution of a statistic is the distribution of its estimate upon infinitely repeated sampling.
- Since we have the full population, we can generate the sampling distribution ourselves!
- (but instead of infinite sampling, we sample 10 000 times)

```{r simulation, cache=TRUE}
means <- numeric(10000)

for (i in 1:10000) {
  
  means[i] <- mean(sumR$height[sample(3e6, size = 100)])
  
}
```


## The sampling distribution: algorithmic

```{r, echo=FALSE}
p <-
  data.frame(x = means) %>%
  ggplot(aes(x = x)) +
  xlim(m - 7, m + 7) +
  geom_histogram(lwd = 1, col = "red", fill = "pink", alpha = 0.7, bins = 80) +
  geom_vline(xintercept = mean(means), lty = 3) +
  labs(x = "Mean height", y = "Count",
       title = "True sampling distribution of the mean") +
  theme_minimal()

plot(p)
```

## Jackknife
In the 50s and 60s, computers enabled another measure of uncertainty

### The Jackknife

- take each person out of your sample once
- calculate the statistic on the remaining values
- the result: 100 slightly different means

```{r jknf, cache=TRUE}
jknf <- numeric(100)

for (i in 1:100) jknf[i] <- mean(sdat100$height[-i])
```

Similar to LOOCV!

## Jackknife
```{r, echo=FALSE}
p <-
  data.frame(x = jknf) %>%
  ggplot(aes(x = x)) +
  xlim(m - 1.25, m + 1) +
  geom_histogram(lwd = 1, col = "seagreen", fill = "lightseagreen", alpha = 0.7, bins = 80) +
  geom_vline(xintercept = m, lty = 3) +
  labs(x = "Mean height", 
       y = "Count", 
       title = "Distribution of Jackknifed means") +
  theme_minimal()

plot(p)
```


## Jackknife
The Jackknife can be used to estimate the standard error:

$$ \begin{align}
\hat{se}_{j} &= \sqrt{\frac{n-1}{n}\sum_{i=1}^n\left(\hat{\theta}_{(i)}-\overline{\hat{\theta}_{(\cdot)}}\right)^2}
\end{align}$$

```{r}
jkse <- sqrt(99/100*sum((jknf - mean(jknf))^2))
```

```{r, echo=FALSE}
cat("Jackknife estimate:", round(jkse, 3), 
    "\nTrue standarderror: 0.750")
```

With this, we can once again create a sampling distribution and a CI based on normal theory.


## Bootstrap
Introduced by Efron in 1979 as computers became more powerful.

- As many times as you like (e.g. 1000 times)
- Sample $n$ values from your sample __with replacement__ 
- Calculate the statistic of interest on this __bootstrap sample__


```{r btstrp, cache=TRUE}
btsp <- numeric(1000)

for (i in 1:1000) btsp[i] <- mean(sdat100$height[sample(100, replace = TRUE)])
```

We sample from our sample, pulling ourselves "up" into the space of population by our bootstraps

## Bootstrap
```{r, echo=FALSE}
p <-
  data.frame(x = btsp) %>%
  ggplot(aes(x = x)) +
  xlim(m - 7, m + 7) +
  geom_histogram(lwd = 1, col = "brown", fill = "orange", alpha = 0.7, bins = 80) +
  geom_vline(xintercept = mean(btsp), lty = 3) +
  labs(x = "Mean height", y = "Count",
       title = "Bootstrap sampling distribution of the mean") +
  theme_minimal()

plot(p)
```

## Bootstrap approximation
```{r, echo=FALSE, fig.height=2.7}
p <-
  data.frame(x = means) %>%
  ggplot(aes(x = x)) +
  xlim(m - 7, m + 7) +
  geom_histogram(lwd = 1, col = "red", fill = "pink", alpha = 0.7, bins = 80) +
  geom_vline(xintercept = mean(means), lty = 3) +
  labs(x = "Mean height", y = "Count",
       title = "True sampling distribution of the mean") +
  theme_minimal()

plot(p)
```
```{r, echo=FALSE, fig.height=2.7}
p <-
  data.frame(x = btsp) %>%
  ggplot(aes(x = x)) +
  xlim(m - 7, m + 7) +
  geom_histogram(lwd = 1, col = "brown", fill = "orange", alpha = 0.7, bins = 80) +
  geom_vline(xintercept = mean(btsp), lty = 3) +
  labs(x = "Mean height", y = "Count",
       title = "Bootstrap sampling distribution of the mean") +
  theme_minimal()

plot(p)
```

## Why?

### No more math!
The standard error calculation does not need to be known

### Relaxes the assumption of normality
For some quantities, unreasonable sample sizes are needed before normality 
(e.g. product of two regression coefficients in mediation analysis)

### Q: What is the sampling distribution of the log mean shoe size?
```{r}
log(mean(sdat100$shoesize))
```

## Log mean shoe size

```{r logmean, cache=TRUE, echo=FALSE}
logm <- numeric(10000)
for (i in 1:10000) logm[i] <- log(mean(sumR$shoesize[sample(3e6, 100)]))
mlogm <- mean(logm)
```

```{r, echo=FALSE, fig.height=2.7}
p <-
  data.frame(x = logm) %>%
  ggplot(aes(x = x)) +
  xlim(mlogm - .01, mlogm + .01) +
  geom_histogram(lwd = 1, col = "dark grey", fill = "grey", alpha = 0.7, bins = 80) +
  geom_vline(xintercept = mean(btsp), lty = 3) +
  labs(x = "Log mean shoe size", y = "Count",
       title = "True sampling distribution of log mean shoe size") +
  theme_minimal()

plot(p)
```

```{r logmean resamp, cache=TRUE, echo=FALSE}
blogm <- numeric(10000)
for (i in 1:10000) 
  blogm[i] <- log(mean(sdat100$shoesize[sample(100, replace = TRUE)]))

```

```{r, echo=FALSE, fig.height=2.7}
p <-
  data.frame(x = blogm) %>%
  ggplot(aes(x = x)) +
  xlim(mlogm - .01, mlogm + .01) +
  geom_histogram(lwd = 1, col = "brown", fill = "orange", alpha = 0.7, bins = 80) +
  geom_vline(xintercept = mean(btsp), lty = 3) +
  labs(x = "Log mean shoe size", y = "Count",
       title = "Bootstrap sampling distribution of log mean shoe size") +
  theme_minimal()

plot(p)
```

## Proportion of humans in LeeuwRden


```{r prop resamp, cache=TRUE, echo=FALSE}
bprp <- numeric(10000)
for (i in 1:10000) 
  bprp[i] <- table(sdat100$city[sample(100, replace = TRUE)])["LeeuwRden"]/100
```

```{r, echo=FALSE}
p <-
  data.frame(x = bprp) %>%
  ggplot(aes(x = x)) +
  xlim(0, .5) +
  geom_histogram(lwd = 1, col = "brown", fill = "orange", alpha = 0.7, bins = 50) +
  geom_vline(xintercept = mean(btsp), lty = 3) +
  labs(x = "Proportion of humans in LeeuwRden", y = "Count",
       title = "Bootstrap sampling distribution of proportion") +
  theme_minimal()

plot(p)
```

## The `boot` package

```{r}
mystat <- function(dat, index) {
  median(dat$height[index])/mean(dat$shoesize[index])
}

bootstat <- sdat100 %>% boot(mystat, R = 1000)

# the first 10 replicates
bootstat$t[1:10]

# SE
sd(bootstat$t)
```
## The `boot` package

```{r}
# 95% CI
boot.ci(bootstat)
```
## The `boot` package
```{r, fig.height=3}
# Distribution
data.frame(mystat = bootstat$t) %>% 
  ggplot(aes(x = mystat)) +
  geom_density(fill = "light seagreen") +
  xlim(4.5, 4.8) +
  theme_minimal()
```

## Conclusion
- Sampling distribution indicates uncertainty
- Bootstrapping approximates the sampling distribution
- Treats sample as population to sample from
- Works when the shape of the sampling distribution is unknown
- Package `boot` can be used for bootstrapping