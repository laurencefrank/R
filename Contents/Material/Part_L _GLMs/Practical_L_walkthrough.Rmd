---
title: "Practical 10"
subtitle: "Generalized Linear Model"
author: "Maarten Cruyff"
params:
  answers: true
output: 
  prettydoc::html_pretty:
    toc: true
    toc_depth: 2
    number_sections: true
    theme: cayman
    highlight: github
---

```{r include = FALSE}
knitr::opts_chunk$set(include = params$answers,
                      comment="")

library(ggplot2)
library(dplyr)
library(GLMsData)
```

---

In this practical, we will compare the estimates of the linear and the 
generalized linear model when the dependent variable is a 
dichotomous variable. The data set for this practical is
`lungcap` from the package `GLMsData`. To load the data into workspace you need 
to use the function `data()`.

```{r}
data(lungcap)
```

# Lungcap

The data set `lungcap` contains data on four characteristics of smokers and 
non-smokers (check its help page for details). In this exercise, we will use 
these characteristics to predict the probability of being a smoker.  

a. Display the summary of the data, and check the variables for skew.

```{r}
summary(lungcap)
```

b. Display a frequency table for the variable `Smoke` to see how many smokers 
and non-smokers there are in the data.

```{r}
xtabs(~ Smoke, lungcap)
```

## Linear model

We will start again by treating the variable `Smoke` as continuous with a 
distribution from the Gaussian family.

a. Fit the generalized linear model `Smoke ~ Age` with the argument 
`family = "gaussian"`, and display and interpret its summary.

```{r}
linear <- glm(Smoke ~ Age, "gaussian", lungcap)
summary(linear)
```

b. Display a scatterplot with `Age` on the x-axis and `Smoke` on the y-axis, and 
add the linear regression line regression line.

```{r}
ggplot(lungcap, aes(Age, Smoke)) + 
  geom_point() + 
  geom_smooth(method = "lm") +
  theme_minimal()
```

c. Display and interpret the diagnostic residual plots for the linear model.

```{r fig.asp = 1}
par(mfrow = c(2, 2))
plot(linear)
```

## Logistic regression model

We will now treat `Smoke` as a dichotomous variable with `glm()`.

a. Fit the logistic regression model `Smoke ~ Age`, and display and interpret 
its summary.

```{r}
logistic <- glm(Smoke ~ Age, "binomial", lungcap)
summary(logistic)
```

b. Display a scatterplot with `Age` on the x-axis and `Smoke` on the y-axis. Add 
a line representing the probability of smoking predicted by the logistic 
regression model, and compare (the range of) the predicted values to those of 
the linear model.

```{r}
lungcap %>% 
  mutate(pred = predict(logistic, type = "response")) %>% 
  ggplot() + 
  geom_point(aes(Age, Smoke)) +
  geom_line(aes(Age, pred), col = "red") +
  theme_minimal()
```

## Model selection

a. Find the most parsimonious main-effects logistic regression model when all 
variables in `lungcap` are allowed to enter the model as predictors.

```{r}
step(logistic, scope = Smoke ~ Age + FEV + Ht + Gender)
```


b. Find the most parsimonious logistic regression model allowing  all pairwise 
interactions between the predictors to eneter the model.

```{r}
step(logistic, scope = Smoke ~ (Age + FEV + Ht + Gender)^2)
```

c. Which models is the most parsimonious, the one with or the one without the pairwise interactions?

---

End of practical
