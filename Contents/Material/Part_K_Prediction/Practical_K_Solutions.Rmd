---
title: "Practical K Solutions"
author: "Maarten Cruyff"
date: "Statistical Programming in R"
output: prettydoc::html_pretty
---

```{r include=F}
knitr::opts_chunk$set(include=T)
```



The data sets `cars` contain the two variables `speed` and `distance`. In this practical we use 5-fold cross validation to check whether the  stopping distance of a car is linearly, qudraticly or cubicly related to the speed of the car. We follow the same steps as in the lecture sheets.

---

1. **Data exploration**

a. Display a plot of the data.

```{r}
plot(cars)
```

b. Display the summary of a cubic model predicting `dist` from `speed`. What do you conclude from this model?

```{r}
summary(lm(dist ~ poly(speed, 3), cars))
```
c. If you display the data you will see that the cases are ordered in ascending order of `speed`. We don't want that we are going to make the folds. To mixed them up a little bit run the following line of code: 

```{r include=T}
set.seed(2)
cars <- cars[sample(1:nrow(cars)), ]
```

---

2. **Initializing variables**

a. Start with making the variable `folds` for 5-fold cross validation.

```{r}
folds <- 5
```

b. Make the list `test_id` by sampling of the indices for the test sets. Display the list to check its content.

```{r}
test_id <- split(1:nrow(cars), 1:5)
test_id
```

3. **Get the MSE for the first fold of the linear model.**

a. Make the matrix `mse_lin` with `folds` rows to store the test MSE's of the linear model.
    
```{r}
mse_lin <- matrix(nrow = folds)
```

b. Fit the linear model to the training set `cars[-test_id[[1]], ]` of the first fold (beware to use the correct number of square brackets!). Save the object as `fit`.
    
```{r}
fit <- lm(dist ~ speed, cars[-test_id[[1]], ])
```
    
c. Get the predictions for the test set and save then as `pred`. Display the result.
    
```{r}
pred <- predict(fit, newdata = cars[test_id[[1]], ])

pred
```

d. Compute the test MS and store it in the 1st row of `pred`.
    
```{r}
mse_lin[1] <- mean((cars$dist[test_id[[1]]] - pred)^2)
```

e. Display `pred`. It should be a matrix with 5 rows with the test MSE of the 1st fold in the first row.
    
```{r}
mse_lin
```

---
    
4. **Programming the loops for the linear, quadratic and cubic models**

a. Write the `for` loop for cross validation of the linear model. Follow the example in the lecture sheets
     (don't forget to change the names of the data set and the variables!). Save the test MSE's in `mse_lin`. Run the loop
     and display the content of `mse_lin`.
     
```{r}
for (i in 1:folds) {
  
  fit        <- lm(dist ~ speed, data = cars[-test_id[[i]], ])      
  
  pred       <- predict(fit, newdata = cars[test_id[[i]], ]) 
  
  mse_lin[i] <- mean((cars$dist[test_id[[i]]] - pred)^2)     # test MSE of the i-th fold
  
}

mse_lin
```
  
b. Now do the same for the quadratic model. Don't forget to make the matrix `mse_quad` to store the test MSE's before
    you run the loop.
     
```{r}
mse_quad <- matrix(nrow = folds)

for (i in 1:folds) {
  
  fit         <- lm(dist ~ poly(speed, 2), data = cars[-test_id[[i]], ])      
  
  pred        <- predict(fit, newdata = cars[test_id[[i]], ]) 
  
  mse_quad[i] <- mean((cars$dist[test_id[[i]]] - pred)^2)     # test MSE of the i-th fold
  
}

mse_quad
```
    
c. And for the cubic model. 
     
```{r}
mse_cub <- matrix(nrow = folds)

for (i in 1:folds) {
  
  fit         <- lm(dist ~ poly(speed, 3), data = cars[-test_id[[i]], ])      
  
  pred        <- predict(fit, newdata = cars[test_id[[i]], ]) 
  
  mse_cub[i]  <- mean((cars$dist[test_id[[i]]] - pred)^2)     # test MSE of the i-th fold
  
}

mse_cub
```

---

5. **Model selection**

a. Display a data frame with the averaged MSE's for the linear, quadratric and cubic models.
     
```{r}
data.frame(linear    = mean(mse_lin), 
           quadratic = mean(mse_quad), 
           cubic     = mean(mse_cub),
           row.names = "Average test MSE")
```

    
b. What is your conclusion regarding the relationship between stopping distance and speed of a car?

c. Run the code again using a different seed than 2 in exercise 1c. Do you reach the same conclusion?
    
---

End of `Practical K`. 
